➜  bin start-dfs.sh
➜  bin hdfs dfs -put /home/quad/Desktop/Assign_6/words.txt input
➜  bin hadoop jar /home/quad/Desktop/Assign_6/Word_Count.jar input output
File System Counters
		FILE: Number of bytes read=8652
		FILE: Number of bytes written=564827
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=262
		HDFS: Number of bytes written=147
		HDFS: Number of read operations=13
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=5
		Map output records=26
		Map output bytes=233
		Map output materialized bytes=291
		Input split bytes=102
		Combine input records=0
		Combine output records=0
		Reduce input groups=21
		Reduce shuffle bytes=291
		Reduce input records=26
		Reduce output records=21
		Spilled Records=52
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=453509120
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=131
	File Output Format Counters 
		Bytes Written=147

➜  bin hdfs dfs -cat output/*
asdf	1
cd	1
ddsds	1
efm	2
emfd	1
erer	1
ew	2
ewew	2
fdfd	1
fdm	1
fds	1
fdsf	1
fefd	2
klm	1
rererw	1
sds	1
wdknsd	2
weewewe	1
werewr	1
wewe	1
xck	1

➜  bin hdfs dfs -rm input
16/03/27 12:23:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted input
➜  bin hdfs dfs -rm -R output
16/03/27 12:23:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted output

